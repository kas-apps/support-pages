# ローカル LLM クライアントの使用方法

このページでは、ローカル LLM クライアントアプリの使用に必要な設定手順を説明します。
**LM Studio サーバーの起動**と、**アプリ側の接続設定**について順を追って解説します。

---

## 🎥 デモ動画（実機操作の様子）

以下の動画で、アプリの操作全体を確認できます。

- アプリでのサーバー設定
- チャット送受信
- チャット履歴の利用・削除

📹 [アプリデモを見る](./videos/app-demo.mp4)

---

## ✅ STEP.1 ローカル LLM サーバーの設定（LM Studio）

### 🎯 目的：

アプリが接続するためのローカルサーバー（LM Studio）を立ち上げ、LAN 内でアクセス可能にします。

### 🔧 手順（PC 側）：

1. LM Studio を起動
2. `Developer` タブをクリック
3. `Status: Stopped` の場合は、`Start server` を ON に切り替え
4. `Settings` を開き、`Serve on Local Network` を ON に変更
5. `Reachable at:` に表示されている **IP アドレス** を控えておく

📹 [PC 設定手順の動画を見る](./videos/pc-settings.mp4)

---

## ✅ STEP.2 アプリの設定（iOS 側）

### 🎯 目的：

ローカル LLM サーバーにアプリから接続できるように設定します。

### 📱 手順（アプリ側）：

1. 画面上部中央の `No Models >` をタップ
2. `API Provider` を `LM Studio` または `Ollama` から選択
3. `Host` 欄に **STEP.1 で確認した IP アドレス** を入力
4. 必要に応じて `Port` 欄にカスタムポート番号を入力（LM Studio のデフォルト: `1234`）
5. `Test Connection` をタップして接続確認
6. 接続が成功したら `Language Model` を選択

---

## 💬 よくある質問（FAQ）

**Q. インターネット接続は必要ですか？**  
A. いいえ、本アプリはローカルネットワーク内で動作し、インターネット接続は不要です。

**Q. チュートリアルはどこから見直せますか？**  
A. 設定画面にある `チュートリアルを見る` ボタンからいつでも再実行できます。

---

## 📝 備考

- このアプリはローカル LLM サーバー（Ollama / LM Studio）を自前で構築できる方向けです。
- 詳細な動作確認については上記の動画をご参照ください。
